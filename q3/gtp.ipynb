{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9526ba-c815-4843-a819-f96015c92e02",
   "metadata": {},
   "source": [
    "# Question 3 - GTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8cdbf-ed84-4bb1-b19d-8c47bcd83ec2",
   "metadata": {},
   "source": [
    "### Stav Cohen 316492776\n",
    "### Ron Kozitsa 312544240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b9d6a7f-6458-4265-9fbf-ca08b3f7a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f40a405-404d-43b8-ad3d-aa9393107dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTP converter class\n",
    "class GtpConverter:\n",
    "\n",
    "    # Fill constants keys in the rules file\n",
    "    def fill_constants(self):\n",
    "        self.PREDECESSOR_KEY = 'predecessor'\n",
    "        self.SUCCESSOR_KEY = 'successor'\n",
    "        self.PHONEMES_KEY = 'phonemes'\n",
    "        self.CENTER_KEY = 'center'\n",
    "        self.RULE_KEY = 'rules'\n",
    "        self.GRAPHEMES_KEY = 'graphemes'\n",
    "        self.SUBSETS_KEY = 'subsets'\n",
    "        self.NAME_KEY = 'name'\n",
    "\n",
    "    # Initialize the GTP converter\n",
    "    def __init__(self, gtp_rules_file):\n",
    "        # Load the rules file\n",
    "        with open(gtp_rules_file, 'r', encoding='utf-8') as f:\n",
    "            self.gtp_rules = json.load(f)\n",
    "            self.fill_constants()\n",
    "            self.fill_subsets()\n",
    "\n",
    "    # Fill the subsets keys in the rules file\n",
    "    def fill_subsets(self):\n",
    "        subset_keys = [subset[self.NAME_KEY] for subset in self.gtp_rules[self.SUBSETS_KEY]]\n",
    "\n",
    "        # Iterate over the rules and fill the subsets keys (e.g SV, EI to their corresponding graphemes)\n",
    "        for rule in self.gtp_rules[self.RULE_KEY]:\n",
    "            if self.PREDECESSOR_KEY in rule and rule[self.PREDECESSOR_KEY] in subset_keys:\n",
    "                rule[self.PREDECESSOR_KEY] = self.gtp_rules[self.SUBSETS_KEY][subset_keys.index(rule[self.PREDECESSOR_KEY])][self.GRAPHEMES_KEY]\n",
    "            \n",
    "            if self.SUCCESSOR_KEY in rule and rule[self.SUCCESSOR_KEY] in subset_keys:\n",
    "                rule[self.SUCCESSOR_KEY] = self.gtp_rules[self.SUBSETS_KEY][subset_keys.index(rule[self.SUCCESSOR_KEY])][self.GRAPHEMES_KEY]\n",
    "            \n",
    "\n",
    "    # Parse the word into chunks\n",
    "    # For example, the word \"sueño\" will be parsed into the following chunks:\n",
    "    # Single chunks: [{$,s,u}, {s,u,e}, {u,e,ñ}, {e,ñ,o}, {ñ,o,$}]\n",
    "    # Double chunks: [{$,su,e}, {s,ue,ñ}, {u,eñ,o}, {e,ño,$}]\n",
    "    def parse_word_into_chunks(self, word: str, number_of_chars=1) -> list[dict]:\n",
    "        # Add $ to the beginning and end of the word\n",
    "        word = f\"${word}$\"\n",
    "\n",
    "        # Iterate over the word and create the chunks\n",
    "        return [{self.PREDECESSOR_KEY: word[i-1],\n",
    "                    self.CENTER_KEY: word[i:i+number_of_chars],\n",
    "                    self.SUCCESSOR_KEY: word[i+number_of_chars]} \n",
    "                for i in range(1, len(word)-number_of_chars)]\n",
    "\n",
    "    # Get the candidates for a given chunk (Where there is a match for the center key)\n",
    "    def get_candidates(self, chunk: dict) -> list[dict]:\n",
    "        return [rule for rule in self.gtp_rules[self.RULE_KEY] if rule[self.CENTER_KEY] == chunk[self.CENTER_KEY]]\n",
    "\n",
    "    # Get the phoneme from the candidates\n",
    "    def get_phonemes_from_candidates(self, chunk: dict, candidates: list[dict]) -> str:\n",
    "        default_candidate = None\n",
    "        \n",
    "        # Iterate over the candidates and get the phoneme\n",
    "        for candidate in candidates:\n",
    "\n",
    "            # If there is a match for the predecessor key, return the phoneme of this candidate\n",
    "            if self.PREDECESSOR_KEY in candidate and chunk[self.PREDECESSOR_KEY] in candidate[self.PREDECESSOR_KEY].split(' '):\n",
    "                return candidate[self.PHONEMES_KEY]\n",
    "\n",
    "            # If there is a match for the successor key, return the phoneme of this candidate\n",
    "            if self.SUCCESSOR_KEY in candidate and chunk[self.SUCCESSOR_KEY] in candidate[self.SUCCESSOR_KEY].split(' '):\n",
    "                return candidate[self.PHONEMES_KEY]\n",
    "\n",
    "            # If there is no match for the predecessor or successor key, \n",
    "            # set the default candidate to the current candidate (the one with no predecessor nor successor)\n",
    "            if self.PREDECESSOR_KEY not in candidate and self.SUCCESSOR_KEY not in candidate:\n",
    "                default_candidate = candidate\n",
    "\n",
    "        # Return the phoneme of the default candidate if it exists, otherwise return None\n",
    "        return default_candidate[self.PHONEMES_KEY] if default_candidate is not None else None\n",
    "\n",
    "    # Process the phonemes for a given single and double chunk\n",
    "    def process_phonemes(self, single_chunk: dict, double_chunk: dict) -> tuple[str, bool]:\n",
    "\n",
    "        # Get the candidates for the double chunk (A matching by 'center' key)\n",
    "        double_candidates = self.get_candidates(double_chunk)\n",
    "\n",
    "        # Get the candidates for the single chunk (A matching by 'center' key)\n",
    "        single_candidates = self.get_candidates(single_chunk)\n",
    "\n",
    "        # If there are double candidates, process the double chunk, otherwise process the single chunk\n",
    "        if len(double_candidates) > 0:\n",
    "\n",
    "            # Get the phoneme from the double candidates\n",
    "            phoneme = self.get_phonemes_from_candidates(double_chunk, double_candidates)\n",
    "            \n",
    "            # If the phoneme of the double chunk is found, return the phoneme and the double chunk indicator to True\n",
    "            # Otherwise, proceed with the single candidates to get the phoneme of the single chunk\n",
    "            if phoneme is not None:\n",
    "                return phoneme, True\n",
    "            else:\n",
    "                phoneme = self.get_phonemes_from_candidates(single_chunk, single_candidates)\n",
    "        else:\n",
    "            phoneme = self.get_phonemes_from_candidates(single_chunk, single_candidates)\n",
    "\n",
    "        # Return the phoneme of the single chunk and the double chunk indicator to False\n",
    "        return phoneme, False\n",
    "\n",
    "\n",
    "    # Process the word into phonemes\n",
    "    def process(self, word: str) -> str:\n",
    "        # Initialize the phonemes list (list of characters of the phonemes of the word)\n",
    "        phonemes = []\n",
    "\n",
    "        # Parse the word into single and double chunks\n",
    "        single_word_chunks = self.parse_word_into_chunks(word, 1)\n",
    "        double_word_chunks = self.parse_word_into_chunks(word, 2)\n",
    "\n",
    "        # Add the end of word chunk to the double chunks in order to make the two chunks lists the same length for the zip function.\n",
    "        double_word_chunks.append({self.CENTER_KEY: '$', self.PREDECESSOR_KEY: '$', self.SUCCESSOR_KEY: '$'})\n",
    "\n",
    "        # Initialize the is_double_chunk flag to False (No double chuck matching detected yet)\n",
    "        is_double_chunk = False\n",
    "\n",
    "        # Iterate over the single and double chunks and process the phonemes\n",
    "        for (single_chunk, double_chunk) in zip(single_word_chunks, double_word_chunks):\n",
    "            \n",
    "            # If the is_double_chunk flag is True (A double chunk has been matched in the previous iteration), skip the current iteration and reset the flag to False\n",
    "            if is_double_chunk:\n",
    "                is_double_chunk = False\n",
    "                continue\n",
    "\n",
    "            # If the single chunk is a valid grapheme, process the phonemes, otherwise print an error message and return None\n",
    "            if single_chunk[self.CENTER_KEY] in self.gtp_rules[self.GRAPHEMES_KEY].split(' '):\n",
    "\n",
    "                # Get the phonemes from the single and double chunks\n",
    "                phoneme, is_double_chunk = self.process_phonemes(single_chunk, double_chunk)\n",
    "\n",
    "                # If the phonemes are found, add them to the phonemes list, otherwise print an error message and return None\n",
    "                if phoneme is not None:\n",
    "                    phonemes.append(phoneme)\n",
    "                else:\n",
    "                    print(f\"No phoneme found for chunk {single_chunk}. Word {word} is not a valid word.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Character {single_chunk['center']} not found in graphemes. Word {word} is not a valid word.\")\n",
    "                return None\n",
    "        \n",
    "        # Return the phonemes as a string\n",
    "        return ''.join(phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd2a5a38-68bf-4fdc-b993-fe6a2807188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp_converter = GtpConverter('spanish_gtp_rules.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6dfcb4-a199-4c70-bc41-7a1ae73d381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sueño: sweɲo\n",
      "pequenita: pekenita\n",
      "desarrollar: desaroʝaɾ\n",
      "guitarra: gitara\n",
      "cigüeña: θigweɲa\n",
      "alburquerque: albuɾkeɾke\n",
      "atenúas: atenuas\n",
      "zorro: θoro\n",
      "muchacho: muʧaʧo\n",
      "hierro: ʝero\n",
      "mándamelo: mandamelo\n",
      "rápidamente: rapidamente\n",
      "chiringuitos: ʧiɾingitos\n",
      "caballeros: kabaʝeɾos\n",
      "escribí: eskɾibi\n"
     ]
    }
   ],
   "source": [
    "list_of_words = [\n",
    "    \"sueño\", \"pequenita\", \"desarrollar\", \"guitarra\", \"cigüeña\", \"alburquerque\", \"atenúas\", \"zorro\",\n",
    "    \"muchacho\", \"hierro\", \"mándamelo\", \"rápidamente\", \"chiringuitos\", \"caballeros\", \"escribí\"\n",
    "]\n",
    "\n",
    "for word in list_of_words:\n",
    "    print(f\"{word}: {gtp_converter.process(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a85baac0-a47a-4e8e-8fcb-c61149972ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "barranquilla: barankiʝa\n",
      "mantequilla: mantekiʝa\n",
      "medellín: medeʝin\n",
      "colombia: kolombja\n",
      "playa: plaʝa\n",
      "estados: estados\n",
      "unidos: unidos\n",
      "ciudad: θjudad\n",
      "méxico: meksiko\n",
      "calle: kaʝe\n",
      "juntos: xuntos\n",
      "sevilla: sebiʝa\n",
      "muñeca: muɲeka\n",
      "año: aɲo\n",
      "javier: xabjeɾ\n",
      "pingüino: pingwino\n",
      "juan: xwan\n",
      "mientras: mjentɾas\n",
      "quiero: kjeɾo\n",
      "querías: keɾias\n",
      "coqueto: koketo\n",
      "trabajar: tɾabaxaɾ\n",
      "hombre: ombɾe\n",
      "aquí: aki\n",
      "porque: poɾke\n",
      "vargüenza: baɾgwenθa\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------\")\n",
    "\n",
    "additional_words = [\n",
    "    \"barranquilla\", \"mantequilla\", \"medellín\", \"colombia\",\"playa\", \"estados\", \"unidos\",\n",
    "    \"ciudad\", \"méxico\", \"calle\", \"juntos\", \"sevilla\", \"muñeca\", \"año\", \"javier\", \"pingüino\",\n",
    "    \"juan\", \"mientras\", \"quiero\", \"querías\", \"coqueto\", \"trabajar\", \"hombre\", \"aquí\", \"porque\", \"vargüenza\"\n",
    "]\n",
    "\n",
    "for word in additional_words:\n",
    "    print(f\"{word}: {gtp_converter.process(word)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
